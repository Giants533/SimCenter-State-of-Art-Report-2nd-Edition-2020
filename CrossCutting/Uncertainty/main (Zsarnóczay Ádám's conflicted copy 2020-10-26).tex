%%%%%%%%%%%%%%%%%%%% author.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample root file for your "contribution" to a contributed volume
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%% Springer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% RECOMMENDED %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\documentclass[graybox]{svmult}
%
%% choose options for [] as required from the list
%% in the Reference Guide
%
%\usepackage{mathptmx}       % selects Times Roman as basic font
%\usepackage{helvet}         % selects Helvetica as sans-serif font
%\usepackage{courier}        % selects Courier as typewriter font
%\usepackage{type1cm}        % activate if the above 3 fonts are
                             % not available on your system
%
%\usepackage{makeidx}         % allows index generation
%\usepackage{graphicx}        % standard LaTeX graphics tool
%                             % when including figure files
%\usepackage{multicol}        % used for the two-column index
%\usepackage[bottom]{footmisc}% places footnotes at page bottom
%
%% see the list of further useful packages
%% in the Reference Guide
%
%\makeindex             % used for the subject index
%                       % please use the style svind.ist with
%                       % your makeindex program
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\begin{document}

\title{Uncertainty Quantification}
% Use \titlerunning{Short Title} for an abbreviated version of
% your contribution title if the original one is too long
\author{
    \textbf{Alexandros Taflanidis}
    \and Joel P. Conte
    \and George Deodatis
    \and Sanjay Govindjee}
\tocauthor{}
\authorrunning{Taflanidis et al.}
% Use \authorrunning{Short Title} for an abbreviated version of
% your contribution title if the original one is too long
%\institute{Name of First Author \at Name, Address of Institute, %\email{name@email.address}
%\and Name of Second Author \at Name, Address of Institute %\email{name@email.address}}
%
% Use the package "url.sty" to avoid
% problems with special characters
% used in your e-mail or web address
%
\maketitle

Uncertainty quantification (UQ) represents one of the fastest evolving scientific fields, with advances in computer science and statistical computing promoting constant developments in the way uncertainty is incorporated in the predictive analysis of engineering systems (Smith, 2013). In particular, over the last decade(s) the popularity of HPC and of machine-learning tools have dramatically impacted the way computational simulation is utilized within the UQ field, lifting many barriers that were traditionally associated with simulation-based UQ techniques, and allowing the detailed characterization and propagation of uncertainties even for problem with highly complex (computationally intensive) numerical models. It is the current consensus within the UQ community that these advances will/have remove(d) the need for simplified approaches with respect to both the uncertainty characterization (assumptions/models used to describe uncertainty and system performance) or uncertainty propagation (estimation of statistics of interest). 

When discussing computational advances and state-of-the art tools in UQ, greater emphasis is typically placed on algorithmic approaches rather than the corresponding software facilitating the implementation of these approaches. The reason for this is that development of scientific tools for UQ has focused traditionally on a specific UQ sub-field [for example, surrogate modeling to support UQ analysis (lophaven2002dacea; gorissen2010surrogate)], with a large number of researchers [for example, Bect et al. (2017) and Clement et al. (2018)] offering open-source algorithms to even address specific class of problems within each of these sub-fields. Although many of these algorithms have been developed in MATLAB, in recent years significant emphasis has been placed on open-source libraries developed using Python and typically distributed though GitHub. 

Since UQ is a very broad field, here discussions focus on applications within the natural hazards engineering field, with some references to relevant general UQ advances also offered. Emphasis is on computational aspects, the most pertinent UQ feature for a state-of-the-art review of UQ simulation methods. Additionally, discussions focus on algorithmic developments, with some references also on relevant software. With respect to description of uncertainty emphasis is placed on probabilistic characterization; even though alternative approaches exist, such as use of fuzzy sets and interval analysis, the current standard of practice in natural hazards engineering is to rely on probabilistic UQ analysis. This can be attributed to the tradition in civil engineering codes to describe performance with respect to statistical measures (probability of exceeding performance limit states), or to the fact that hazard exposure, the most significant source of variability when discussing risk in the natural hazards engineering context, is almost always described using probabilistic measures (mcguire2004seismic; Resio et al., 2007). 

All references provided are merely indicative ones (though a few of them can be regarded as recent seminal work), since the field is very broad and constantly expanding.

\section{Uncertainty Characterization}
\label{sec:uq_characterization}

In natural hazards engineering, characterization of the uncertainties impacting predictions is integrally related to risk quantification. Performance based engineering (PBE) (goulet2007evaluation; Riggs et al., 2008; ciampoli2011performancebased; barbato2013performancebased) represents undoubtedly the foundational development for this task. Performance-based engineering decouples the risk quantification to its different components, mainly hazard analysis (exposure), structural analysis (vulnerability), and damage and loss analysis (consequences), with uncertainties included (and impacting predictions) in all these components. Variability of the hazard itself, in terms of both occurrence and intensity, is widely acknowledged to correspond to the most significant source of uncertainty in this setting. Frequently hazard variability is represented through a resultant IM (baker2005vectorvalued; kohrangi2016implications), though comprehensive approaches that focus on connecting the excitation to parameters of the geophysical process creating it also exist, for example in earthquake engineering description of time-histories through use of stochastic ground motion models dependent on seismological parameters (bijelic2018validation; vlachos2018predictive) or in coastal risk estimation use of surge modeling numerical tools dependent on atmospheric storm characteristics (Resio et al., 2007). Beyond the hazard variability, uncertainties related to parameters of the structural model or generalized system model (for applications not examining directly structural risk) and to the characteristics for describing performance are also recognized as important for inclusion in risk estimation (Porter et al., 2002). The term “system” will be used herein to describe the application of interest; this may pertain, for example, to a building model, to an infrastructure network, or to a soil–structure interaction system configuration. 

Uncertainty within this natural hazards engineering risk characterization setting is ultimately described through a discrete number of parameters (random variables) pertaining to either the hazard or the system/performance model. Even when the uncertainty description for the underlying problem actually entails a stochastic sequence or a random field, a discretized approximation of these functions is commonly utilized, as necessitated by the numerical tools used to compute the system response (gidaris2014surrogate). This translates into use of a parameterized realization for the excitation or model characteristics, an approach that seamlessly fits within the overall PBE framework. Exceptions exist primarily for stochastic dynamics problems, for which propagation of the stochastic excitation uncertainty can be performed using random vibration theory, such as exact or approximate solution of stochastic differential equations or estimation of stationary statistics in the frequency domain \citep{li2009stochastic}. Though such approaches offer substantial benefits, their implementation is primarily constrained to linear systems or nonlinear systems with moderate degree of complexity (dos dossantos2016incremental; wang2016tailequivalent), such as systems with very small number of degrees-of-freedom or nonlinearities having simple, analytical form. As such, their utility within natural hazards engineering is limited to specialized applications. Even in such cases, the remaining uncertainties, beyond the stochastic excitation itself, must be described using a parametric description. The overall parameterized uncertainty description promoted within PBE is therefore well aligned with such approaches, as their adoption simply requires substitution of the deterministic simulation system model with a stochastic simulation system model, the latter representing the solution of the stochastic dynamics problem.

When using Monte Carlo simulation techniques to propagate uncertainty (see discussion in the next paragraph), a critical part of the methodology is the numerical generation of sample functions of the stochastic processes, fields, and waves involved in the problem, modeling the uncertainties in the excitations (e.g., wind velocities, seismic ground motion, and ocean waves) and in the structural system (e.g., material and geometric properties). These processes, fields, and waves can be stationary or non-stationary, homogeneous, or non-homogeneous, scalar or vector, 1D or multi-dimensional, Gaussian or non-Gaussian, or any combination of the above. It is crucial for a simulation algorithm to be computationally efficient as a very large number of sample functions might be needed. A wide range of methodologies is currently available to parametrically describe uncertainty and perform these simulations, including the spectral representation method, Karhunen-Loeve expansion, polynomial chaos decomposition, auto-regressive moving-average models, local average subdivision method, wavelets, Hilbert transform techniques, and turning bands method.

The setting outlined in the previous two paragraphs leads, ultimately, to risk characterized as a multidimensional integral over the parametric uncertainty description (input), with uncertainty propagation (output) translating to estimation of the relevant statistics (estimation of integrals representing moments or reliability with respect to different limit states). The aforementioned integral is frequently expressed with respect to the conditional distributions of the different resultant risk components (goulet2007evaluation; barbato2013performancebased), for example {hazard / response given hazard / consequences given response}. This represents merely a simplification for risk quantification purposes as allows for the decoupling of the different components. Even when this simplification is invoked, risk ultimately originates from the uncertainty in the model parameters of the system, quantified by assigning a probability distribution to them, representing the UQ input. 

\section{Uncertainty Propagation}
\label{sec:uq_propagation}

For uncertainty propagation, the traditional approach in natural hazards engineering has been the use of point estimation methods, either methods that focus on the most probable values of the model parameters like the first-order second moment (FOSM) method \citep{baker2008uncertainty} and its variants (vamvatsikos2013derivation), or methods that focus on the peaks of the integrand of the probabilistic integral (design points) like the first- and second-order reliability methods (FORM/SORM) \citep{koduru2010feasibility}. As point estimation methods are inherently approximate, with no available means to control their accuracy, advances in computer science and statistics have encouraged researchers the past decade to rely more heavily on Monte Carlo simulation tools for uncertainty propagation in natural hazards engineering (Smith and Caracoglia, 2011; Taflanidis and Jia, 2011; vamvatsikos2014seismic; esposito2015simulation). 

Although point estimation methods do still maintain utility and popularity, natural hazards engineering trends follow the broader UQ community trends in promoting computer and Monte Carlo simulation approaches, as these techniques facilitate high-accuracy uncertainty propagation (unbiased estimation) with no fundamental constraints on the complexity of the probability and numerical models used. Of course, computational complexity is still a concern for Monte Carlo simulation. The current state of the art in natural hazards engineering for addressing this challenge is to leverage both advanced Monte Carlos simulation techniques (li2017system; bansal2018subset) and, more importantly, machine-learning and computational statistics tools (Abbiati et al., 2017; ding2018multifidelity; Su et al., 2018; wang2018bayesian). Relevant recent advances for Monte Carlo simulations focus on variance reduction techniques and rare-event simulation, with some emphasis on problems with high-dimensional uncertainties, whereas for machine learning, focus is primarily on use of a variety of surrogate modeling (metamodeling) techniques. Most machine learning implementations in natural hazards engineering fall under the category of direct adoption of techniques developed by the broader UQ community, though a number of studies do address challenges unique to the integration of surrogate modeling in natural hazards engineering problems, for example, the need to address the high-dimensionality of input when a stochastic description is utilized for non-stationary excitations (gidaris2015kriging).

Note: the natural hazards engineering modeling community has been continuously increasing the complexity of the models they adopt. Such high-fidelity numerical models, able to capture the behavior of structural, geotechnical, and soil–foundation-structural systems (all the way to collapse or the brink of collapse) are inherently nonlinear hysteretic (path-dependent) and frequently degrading/softening; therefore, they present (significant) challenges in term of robustness of convergence of the iterative schemes used to integrate their equations of motion. The significance of these challenges will further increase in the context of Monte Carlo simulation-based UQ and requires significant research efforts to be overcome.

Discussing more broadly advances in the UQ field, emphasis is currently strongly placed on machine learning techniques for accelerating UQ computations (Murphy, 2012; ghanem2017handbook; Tripathy and Bilionis, 2018). The relevant developments are frequently integrated with advanced Monte Carlo simulation techniques, particularly for simulation of rare events (li2011efficient; balesdent2013krigingbased; bourinet2016rareevent). With respect directly to machine learning, some emphasis is given on approaches for tuning and validation (mehmani2018concurrent), though the primary focus is on the proper design of the computer simulation experiments (DoE) (kleijnen2008design; Picheny et al., 2010) that are used to inform the development of the relevant computational statistics tools. Adaptive DoE is widely acknowledged to offer substantial advantages in balancing computational efficiency and accuracy for UQ analysis when machine-learning techniques are used, and significant research efforts are currently focused on advancing DoE techniques; however, it remains an open challenge for the community. 

The concept of model fidelity remains unexplored within the natural hazards engineering community, but it plays a central role in modern UQ techniques, with a range of algorithms developed to properly integrate hierarchical fidelity models to promote efficient and accurate uncertainty propagation (geraci2017multifidelity; Peherstorfer et al., 2018). Combination of machine-learning (primarily surrogate modeling) techniques with different fidelity models is also a topic that has been receiving increasing attention for facilitating the use of expensive numerical models in UQ (de Baar et al., 2015; zhou2016active). In the natural hazards engineering setting, discussions on explicitly exploiting model fidelity for risk estimation are very limited; therefore, the community still heavily emphasizes use of high-fidelity models without, yet, examining how different levels of simulation fidelity and the use of reduced order models can be properly combined to promote efficient and accurate risk estimation. Multi-fidelity Monte Carlo and hierarchical surrogate modeling techniques constitute, undoubtedly, important opportunity areas for advancing UQ analysis in natural hazards engineering.

Another important aspect of uncertainty propagation is the concept of sensitivity analysis. In natural hazards engineering, this has been primarily implemented as local sensitivity analysis (i.e., estimation of gradient information) since this fits well with the point estimation methods used frequently for calculation of statistics, which aids in the identification of design points. Of greater importance within a UQ setting is a global sensitivity analysis (Sobol, 1990; Saltelli, 2002; Rahman, 2016) that allows identification of the relative importance of the different sources of uncertainty, offering insights with respect to both accelerating UQ computations as well as to understanding of the critical factors impacting the overall risk. Though global sensitivity analyses can be particularly useful for hazard applications \citep{vetter2012global}, it is currently receiving limited practical interest within natural hazards engineering [though implementations do exist even for all purpose codes; see Bourinet et al. (2009)]. More formal integration of global sensitivity analysis tools within the natural hazards engineering community represents another topical area where advancements can/should be made. Note: the computational cost for global sensitivity analysis, e.g., calculation of first and higher order sensitivity indexes, is much higher than the cost of simple uncertainty propagation; relevant techniques range from use of quasi-Monte Carlo (Saltelli, 2002) to surrogate modeling (Sudret, 2008) to sample-based methods relying on approximation of conditional distributions (Jia and Taflanidis, 2016; li2016efficient).

\section{Model Calibration and Bayesian Inference}
\label{sec:uq_calibration}

Model updating/calibration plays an important role in natural hazards engineering, with data coming from both component (or system)–level experiments or system–level observations during (or post) actual excitation conditions. Within a UQ setting, the current standard to perform this updating is Bayesian inference (beck2010bayesian; kontoroupi2017online). Using observation data, Bayesian inference can be leveraged to provide different type of outputs/results \citep{beck2013prior} through the following three tasks: identify the most probable model parameters or even update the entire probability density function for these parameters (obtain posterior distributions); perform posterior predictive analysis, and update risk using the new information; when different numerical models are examined, identify the probability of each of them (as inferred by the data) to either select the most appropriate or calculate the weights when all of them will be used in a model averaging setting (model class selection). The typical implementation refers to model parameter updating, what is traditionally viewed as model calibration, with model class selection less frequently used, especially within natural hazards engineering community applications. Still, Bayesian model class selection offers a comprehensive tool for evaluating appropriateness of different models \citep{muto2008bayesian}, and especially for natural hazards engineering applications that can be integrated with health monitoring tools \citep{oh2018bayesian}.

From a computational perspective, Bayesian updating can incur computational burden, especially when complex FEM models are utilized and a variety of approaches are used to address this challenge. Common approaches include the use of advanced MCS techniques to reduce the total number of simulations needed (Quiroz et al., 2018), the integration of metamodeling to approximate the complex system model (angelikopoulos2015xtmcmc), or the use of direct differentiation tools to accelerate computations (astroza2017batch). Bayesian updating may rely on point estimates, which are equivalent to identifying and using only the most probable  model parameter values (based on the observation data)—expressed as a nonlinear optimization problem—or leveraging the entire posterior distribution—expressed as a problem of sampling from this distribution.

For the latter, Markov Chain Monte Carlo (MCMC) techniques need to be used for any of the three tasks entailed in Bayesian inference (catanach2018bayesian). For problems involving inference for dynamical models (germane to the majority of applications in natural hazards engineering), updating can be done in batch mode that uses all observation data or recursive mode that sequentially updates model characteristics during the time history for the observations (astroza2017batch). The batch approach is a direct implementation of the broader Bayesian inference framework. The recursive implementation typically leads to filtering approaches, including Kalman filters (KF) and its variants (Extended KF or Unscented KF), that rely on linear or Gaussian assumptions (astroza2017batch; kontoroupi2017online; erazo2018bayesian), and particle filters (PF) that rely on a sequential MCS approach and do not involve any type of (linear/Gaussian) assumptions (chatzi2009unscented; wei2013dynamic; Olivier and Smyth, 2017). The recursive approach is used primarily for real-time or online applications and focuses primarily on the most probable parameter values. 

\section{Design under Uncertainty}
\label{sec:uq_design}

In natural hazards engineering, design under uncertainty has been traditionally expressed as a reliability-based design optimization (RBDO) (Spence and Gioffrè, 2012; chun2019systemreliabilitybased) or as a robust design optimization (RDO) (greco2015robust) problem. Some recent approaches deviate from this pattern and follow directly PBE advances by formulating the design problem with respect to life-cycle cost and performance objectives (Shin and Singh, 2014), and even adopting multiple probabilistic criteria to represent different risk-attitudes (gidaris2017multiobjective). Practical applications focus on design of supplemental dissipative devices (Shin and Singh, 2014; gidaris2017multiobjective; Altieri et al., 2018) and member-sizing (huang2015performancebased; Suksuwan and Spence, 2018), and in some cases on topology-based optimization of structural systems (bobby2017reliabilitybased; Yang et al., 2017).

With respect to the solution of the corresponding optimization problem, the natural hazards engineering community follows the broader UQ trends. Design under uncertainty optimization problems undoubtedly present significant computational challenges since they combine two tasks, each with considerable computational burden: uncertainty propagation and optimization. Discussed next is how uncertainty propagation is handled within this coupled problem.

Common approaches, especially within context of RBDO and RDO, typically rely on approximate point-estimation methods like FORM/SORM (Papadimitriou et al., 2018) and some sort of decoupling of the optimization/uncertainty-propagation loops to accelerate convergence (beyer2007robust). Over the past decade, advances in the use of simulation techniques within UQ have created new opportunities that incorporate MCS techniques to solve design-under-uncertainty problems (Spall, 2003; li2016typhoon), thereby lifting some of the traditionally associated computational barriers. Greater emphasis is continuously being placed on solving design-under-uncertainty problems using advanced Monte Carlo techniques (medina2014adaptive), which are frequently coupled with an intelligent integration of surrogate modeling tools (zhang2018adaptive). It is expected that this trend will continue since computer science and machine-learning advances have dramatically altered the computational complexity for leveraging MCS for design optimization under uncertainty, offering an attractive alternative to traditional approaches that relied on the approximate (but highly efficient) point estimation methods. 

\section{Relevant Software}
\label{sec:uq_tools}

Beyond specific UQ algorithms developed by individual researchers and shared in repositories like GitHub or MATLAB’s File Exchange, two other important UQ software categories exist:

\begin{itemize}
    \item Libraries integrated with existing modeling tools appropriate for natural hazards engineering analysis, like the general purpose FEM reliability tools offered through FERUM (bourinet2009review). These libraries frequently address a specific type of UQ analysis, e.g., direct MCS or reliability estimation.
    \item Software that looks at UQ analysis with a broad brush could be appropriate for use in natural hazards engineering applications (but as of yet has not necessarily been developed specifically for that purpose). Such software typically covers the entire range of UQ analysis, with continuous integration of the relevant state-of-the art advances. They are composed of scientific modules that perform different UQ tasks, connected through the main software engine and, in addition, are commonly equipped with an appropriate GUI. 
\end{itemize}
	
The last category of UQ software packages is of greater interest, especially since it covers the entire domain of a rapidly expanding field and facilitates the integration of the relevant developments, which typically leverage different classes of tools (e.g., rare-event simulation using surrogate models with adaptive refinement. UQ software programs typically address the following tasks:

\begin{itemize}
    \item Probabilistic modeling. This pertains to standard uncertainty characterization, extending from simple parametric description to stochastic characterization and represents the input to the UQ software.
    \item Monte Carlo and reliability analysis, extending from direct MCS with Latin hypercube sampling, to use of point estimation methods (FORM/SORM), to variance reduction, to rare event simulation. UQ outputs considered correspond typically to statistical moments, probabilities of exceedance for different limit states, or fitted distributions. The numerous software programs adopt different tools for the aforementioned tasks and most lack a complete adaptive implementation; some degree of competency on behalf of the end-user for selecting appropriate algorithms and parameters is assumed. Many types of software have started recently to integrate multi-fidelity MCS approaches.
    \item Surrogate modeling. Common classes of metamodels used include Gaussian processes, polynomial chaos, support vector machines, and radial basis functions. The developed surrogate models can be then leveraged within the software to accelerate computations for other UQ tasks. Adaptive DoE options are typically available and used frequently; standard DoE approaches are not, however, necessarily tailored to the specific UQ task the end-user is interested in applying. Most software programs sacrifice robustness (an approach that is reliable and works independent of the end-user competency) for efficiency (the ability to develop high-accuracy metamodels with the least number of simulation experiments).
    \item Global sensitivity analysis. This is typically performed through calculation of Sobol indices (UQ output) using some approximate (quasi-Monte Carlo) technique or surrogate modeling (polynomial chaos expansion). 
    \item Data analysis and model calibration, with some emphasis on Bayesian inference techniques. Although Bayesian updating is very common, model class selection is not. Addressing modeling complexity remains a bigger challenge for Bayesian inference applications since integrating metamodeling techniques is not trivial. The challenge here is to establish a fully automated integration that can address different degrees of competency for the end-user and a wide range of application problems with certain degree of robustness (impact of metamodel error). For problems with dynamical models, the typical approach is to use the batch updating method since that leads to a common broader Bayesian inference framework. 
    \item Design-under-uncertainty. Though this is not a common option, some software programs do offer the ability to perform some form of optimization under uncertainty and are most applicable to RBDO and RDO problems. Integrating state-of-the-art MCS techniques in this setting remains also a challenge. Implementations are typically computationally expensive or rely on approximate approaches for the uncertainty propagation.
\end{itemize}

\noindent Out of the different UQ software programs that exist, the following are worth direct mention as representing the state-of-the-art: 
\newline

\noindent\textbf{DAKOTA (https://dakota.sandia.gov/)} \\Developed by the Sandia National Laboratory and written in C++, DAKOTA is widely considered as the standard for UQ software and delivers both state-of-the-art research and robust, usable tools for optimization and UQ (Adams et al., 2009). It has a range of algorithms for all aforementioned UQ tasks and has a wide community that supports its continuous development. 
\newline

\noindent\textbf{OpenTURNS (http://www.openturns.org/)} \\It is an open-source (C++/Python library) initiative for the treatment of uncertainties and risk in simulations (Andrianov et al., 2007). It addresses all aforementioned UQ tasks apart from design under uncertainty. 
\newline

\noindent\textbf{UQLab (https://www.uqlab.com/)} \\Developed at ETH Zürich, UQLAB is a MATLAB-based general purpose UQ framework (marelli2014uqlab). Like OpenTURNS it addresses all the aforementioned UQ tasks apart from design under uncertainty. 
\newline

\noindent\textbf{OpenCossan (http://www.cossan.co.uk/software/open-cossan-engine.php)} \\It is the MATLAB based open-source version of the commercial software COSSAN-X (Patelli et al., 2017), which was initially developed to integrate UQ and reliability techniques within FEM analysis, with modules that extend across all aforementioned UQ tasks.

Beyond these specific software, there are a constantly increasing number of Python-based open-source package libraries offered by researchers for UQ analysis; e.g., UQ-Pyl (http://www.uq-pyl.com/) and UQpy (https://github.com/SURGroup/UQpy/).
